# -*- coding: utf-8 -*-
"""Generate Data & Accuration TA (7.30 - 17.30) 9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ju61gsdbyp2LY79ZlKfPXYhKZoC1_rXg
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install keras scikit-learn imbalanced-learn

pip install --upgrade scikit-learn

import pandas as pd
from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()

df = pd.read_csv("/content/drive/MyDrive/Dataset TA/DATA 80% timestamp.csv", sep=';', header=0)
print(df)

"""# TRAINING DATA

# AVERAGING ENSEMBLE METHOD
"""

#AVERAGING 

from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()

# importing utility modules
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
 
# importing machine learning models for prediction
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.linear_model import LinearRegression

# loading train data set in dataframe from train_data.csv file
df = pd.read_csv("/content/drive/MyDrive/Dataset TA/DATA 80% timestamp.csv", sep=';', header=0)

# getting target data from the dataframe
target = df["Timestamp"]
 
# getting train data from the dataframe
train = df.drop(["Timestamp"], axis=1)
 
# Splitting between train data into training and validation dataset
X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.20)
 
# initializing all the model objects with default parameters
model_1 = LinearRegression()
model_2 = xgb.XGBRegressor()
model_3 = RandomForestRegressor()
 
# training all the model on the training dataset
model_1.fit(X_train, y_train)
model_2.fit(X_train, y_train)
model_3.fit(X_train, y_train)
 
# predicting the output on the validation dataset
pred_1 = model_1.predict(X_test)
pred_2 = model_2.predict(X_test)
pred_3 = model_3.predict(X_test)
 
# final prediction after averaging on the prediction of all 3 models
pred_final = (pred_1+pred_2+pred_3)/3.0
 
# printing the mean squared error between real value and predicted value
print(mean_squared_error(y_test, pred_final))

import sklearn
print(sklearn.__version__)

from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()

# importing utility modules
from sklearn.model_selection import train_test_split
 
# importing machine learning models for prediction
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV

# loading train data set in dataframe from train_data.csv file
df = pd.read_csv("/content/drive/MyDrive/Dataset TA/DATA 80% timestamp.csv", sep=';', header=0)

# getting target data from the dataframe
target = df["Timestamp"]
 
# getting train data from the dataframe
train = df.drop(["Timestamp"], axis=1)
 
# Splitting between train data into training and validation dataset
X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.20)

DecisionTree = DecisionTreeClassifier()
KNN = KNeighborsClassifier(n_neighbors=2)
MLPC = MLPClassifier(random_state=1, max_iter=300)
RandomForest = RandomForestClassifier()
XGB = XGBClassifier()

#fit a base model
vc = VotingClassifier([('dt', DecisionTree), 
                       ('KNN', KNN), 
                       ('MLPC', MLPC), 
                       ('rf', RandomForest), 
                       ('xgb', XGB)])
cvm = cross_val_score(vc, X_train, y_train)
base_score = cvm.mean()

print(base_score)

"""AVERAGING TUNING"""

#set parameters
params = {'voting':['hard', 'soft'],
          'weights':[(1,1,1,1,1), (2,1,1,1,1), 
                     (1,2,1,1,1), (1,1,2,1,1),
                     (1,1,1,2,1), (1,1,1,1,2), 
                     (1,1,1,2,2), (2,1,1,1,2)]}
#fit gridsearch & print best params
grid = GridSearchCV(vc, params)
grid.fit(X_train, y_train)
print('\n')
print(f'The best params is : {grid.best_params_}')

#print the final cv score
tuned_vc = VotingClassifier([('dt', DecisionTree), 
                             ('KNN', KNN), 
                             ('MLPC', MLPC), 
                             ('rf', RandomForest), 
                             ('xgb', XGB)], 
                            **grid.best_params_, n_jobs = -1)

tuned_cvm = cross_val_score(tuned_vc, X_train, y_train)
tuned_score = tuned_cvm.mean()

print(round(tuned_score, 4))

"""# BOOSTING ENSEMBLE METHOD"""

#BOOSTING

from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()

# importing utility modules
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
 
# importing machine learning models for prediction
from sklearn.ensemble import GradientBoostingRegressor
 
# loading train data set in dataframe from train_data.csv file
df = pd.read_csv("/content/drive/MyDrive/Dataset TA/DATA 80% timestamp.csv", sep=';', header=0)

# getting target data from the dataframe
target = df["Timestamp"]
 
# getting train data from the dataframe
train = df.drop(["Timestamp"], axis=1)
 
# Splitting between train data into training and validation dataset
X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.20)
 
# initializing the boosting module with default parameters
model = GradientBoostingRegressor()
 
# training the model on the train dataset
model.fit(X_train, y_train)
 
# predicting the output on the test dataset
pred_final = model.predict(X_test)
 
# printing the mean squared error between real value and predicted value
print(mean_squared_error(y_test, pred_final))

"""BOOSTING TUNING"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform as sp_randFloat
from scipy.stats import randint as sp_randInt

model = GradientBoostingRegressor()
parameters = {'learning_rate' : sp_randFloat(),
              'subsample'     : sp_randFloat(),
              'n_estimators'  : sp_randInt(100, 1000),
              'max_depth'     : sp_randInt(4, 10)}    

randm = RandomizedSearchCV(estimator=model, param_distributions=parameters,
                           cv=2, n_iter=10, n_jobs=-1)

randm.fit(X_train, y_train)

print(randm.best_score_)

"""Recall, Precision, F1 Score"""

from sklearn import datasets
from sklearn import metrics
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

# loading train data set in dataframe from train_data.csv file
df = pd.read_csv("/content/drive/MyDrive/Dataset TA/DATA 80% timestamp.csv", sep=';', header=0)

# getting target data from the dataframe
target = df["Timestamp"]
 
# getting train data from the dataframe
train = df.drop(["Timestamp"], axis=1)
 
# Splitting between train data into training and validation dataset
X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.20)

model_GBC = GradientBoostingClassifier()
model_GBC.fit(X_train, y_train)
print(model_GBC)

expected_y  = y_test
predicted_y = model_GBC.predict(X_test)

print(metrics.classification_report(expected_y, predicted_y))
print(metrics.confusion_matrix(expected_y, predicted_y))

model_GBR = GradientBoostingRegressor()
model_GBR.fit(X_train, y_train)
print(model_GBR)

expected_y  = y_test
predicted_y = model_GBR.predict(X_test)

print(metrics.r2_score(expected_y, predicted_y))
print(metrics.mean_squared_log_error(expected_y, predicted_y))
    
plt.figure(figsize=(10,10))
sns.regplot(expected_y, predicted_y, fit_reg=True, scatter_kws={"s": 100})

"""# Bagging Ensemble Method"""

# Bagging

# importing utility modules
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# importing machine learning models for prediction
import xgboost as xgb

# importing bagging module
from sklearn.ensemble import BaggingRegressor

# loading train data set in dataframe from train_data.csv file
df = pd.read_csv("/content/drive/MyDrive/Dataset TA/DATA 80% timestamp.csv", sep=';', header=0)

# getting target data from the dataframe
target = df["Timestamp"]
 
# getting train data from the dataframe
train = df.drop(["Timestamp"], axis=1)

# Splitting between train data into training and validation dataset
X_train, X_test, y_train, y_test = train_test_split(
	train, target, test_size=0.20)

# initializing the bagging model using XGboost as base model with default parameters
model = BaggingRegressor(base_estimator=xgb.XGBRegressor())

# training model
model.fit(X_train, y_train)

# predicting the output on the test dataset
pred = model.predict(X_test)

# printing the mean squared error between real value and predicted value
print(mean_squared_error(y_test, pred_final))

"""Recall, Precision, F1 SCore"""

from sklearn import datasets
from sklearn import metrics
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import BaggingRegressor
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

# loading train data set in dataframe from train_data.csv file
df = pd.read_csv("/content/drive/MyDrive/Dataset TA/DATA 80% timestamp.csv", sep=';', header=0)

# getting target data from the dataframe
target = df["Timestamp"]
 
# getting train data from the dataframe
train = df.drop(["Timestamp"], axis=1)
 
# Splitting between train data into training and validation dataset
X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.20)

model_BC = BaggingClassifier()
model_BC.fit(X_train, y_train)
print(model_BC)

expected_y  = y_test
predicted_y = model_BC.predict(X_test)

print(metrics.classification_report(expected_y, predicted_y))
print(metrics.confusion_matrix(expected_y, predicted_y))

model_BR = BaggingRegressor()
model_BR.fit(X_train, y_train)
print(model_BR)

expected_y  = y_test
predicted_y = model_BR.predict(X_test)

print(metrics.r2_score(expected_y, predicted_y))
print(metrics.mean_squared_log_error(expected_y, predicted_y))
    
plt.figure(figsize=(10,10))
sns.regplot(expected_y, predicted_y, fit_reg=True, scatter_kws={"s": 100})

"""# TESTING DATA"""

#BOOSTING

from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()

# importing utility modules
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
 
# importing machine learning models for prediction
from sklearn.ensemble import GradientBoostingRegressor
 
# loading train data set in dataframe from train_data.csv file
df = pd.read_csv("/content/drive/MyDrive/Dataset TA/DATA 20% timestamp.csv", sep=';', header=0)

# getting target data from the dataframe
target = df["Timestamp"]
 
# getting train data from the dataframe
train = df.drop(["Timestamp"], axis=1)
 
# Splitting between train data into training and validation dataset
X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.20)
 
# initializing the boosting module with default parameters
model = GradientBoostingRegressor()
 
# training the model on the train dataset
model.fit(X_train, y_train)
 
# predicting the output on the test dataset
pred_final = model.predict(X_test)
 
# printing the mean squared error between real value and predicted value
print(mean_squared_error(y_test, pred_final))





"""# MAX VOTING ENSEMBLE METHOD


"""

# importing utility modules
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
from sklearn.metrics import log_loss
 
# importing machine learning models for prediction
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
 
# importing voting classifier
from sklearn.ensemble import VotingClassifier
 
# loading train data set in dataframe from train_data.csv file
df = pd.read_csv("/content/drive/MyDrive/Dataset TA/DATA 80% timestamp.csv", sep=';', header=0)

# getting target data from the dataframe
target = df["Timestamp"]
 
# getting train data from the dataframe
train = df.drop(["Timestamp"], axis=1)
 
# Splitting between train data into training and validation dataset
X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.20)
 
# initializing all the model objects with default parameters
model_1 = LogisticRegression()
model_2 = XGBClassifier()
model_3 = RandomForestClassifier()
 
# Making the final model using voting classifier
final_model = VotingClassifier(
    estimators=[('lr', model_1), ('xgb', model_2), ('rf', model_3)], voting='hard')
 
# training all the model on the train dataset
final_model.fit(X_train, y_train)
 
# predicting the output on the test dataset
pred_final = final_model.predict(X_test)
 
# printing log loss between actual and predicted value
print(log_loss(y_test, pred_final))

# MAX VOTING

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split

from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()

# loading train data set in dataframe from train_data.csv file
df = pd.read_csv("/content/drive/MyDrive/Dataset TA/DATASET TIMESTAMP REVISI.csv", sep=';', header=0)
 
# getting target data from the dataframe
target = df["TIMESTAMP"]
 
# getting train data from the dataframe
train = df.drop(["TIMESTAMP"], axis=1)
 
# Splitting between train data into training and validation dataset
X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.20)

lr = LogisticRegression(solver='liblinear', random_state=1)
rf = RandomForestClassifier(max_features=None, n_jobs=4, random_state=1)
dt = DecisionTreeClassifier(criterion='entropy', max_features='sqrt')

# create ensemble of 3 classifiers
vc = VotingClassifier([('clf1', lr), ('clf2', rf), ('clf3', dt)])
cross_val_score(vc, X_train, y_train).mean()

"""MAX VOTING TUNING"""

# define VotingClassifier parameters to search
param = {'voting': ['hard', 'soft'],
         'weights': [(1,1,1), (2,1,1), (1,2,1), (1,1,2)]}

grid = GridSearchCV(vc, param)
grid.fit(X_train, y_train)
grid.best_score_

"""# STACKING"""

pip install vecstack

# Stacking

from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()

# importing utility modules
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# importing machine learning models for prediction
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.linear_model import LinearRegression

# importing stacking lib
from vecstack import stacking

# loading train data set in dataframe from train_data.csv file
df = pd.read_csv("/content/drive/MyDrive/Dataset TA/DATA 80% timestamp.csv", sep=';', header=0)

# getting target data from the dataframe
target = df["Timestamp"]
 
# getting train data from the dataframe
train = df.drop(["Timestamp"], axis=1)

# Splitting between train data into training and validation dataset
X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.20)

# initializing all the base model objects with default parameters
model_1 = LinearRegression()
model_2 = xgb.XGBRegressor()
model_3 = RandomForestRegressor()

# putting all base model objects in one list
all_models = [model_1, model_2, model_3]

# computing the stack features
s_train, s_test = stacking(all_models, X_train, X_test,	y_train, regression=True, n_folds=4)

# initializing the second-level model
final_model = model_1

# fitting the second level model with stack features
final_model = final_model.fit(s_train, y_train)

# predicting the final output using stacking
pred_final = final_model.predict(X_test)

# printing the mean squared error between real value and predicted value
print(mean_squared_error(y_test, pred_final))

"""# BLENDING"""

# importing utility modules
import pandas as pd
from sklearn.metrics import mean_squared_error

# importing machine learning models for prediction
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.linear_model import LinearRegression

# importing train test split
from sklearn.model_selection import train_test_split

# loading train data set in dataframe from train_data.csv file
df = pd.read_csv("/content/drive/MyDrive/Dataset TA/DATA 80% timestamp.csv", sep=';', header=0)

# getting target data from the dataframe
target = df["Timestamp"]
 
# getting train data from the dataframe
train = df.drop(["Timestamp"], axis=1)

#Splitting between train data into training and validation dataset
X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.20)

# performing the train test and validation split
train_ratio = 0.70
validation_ratio = 0.20
test_ratio = 0.10

# performing train test split
x_train, x_test, y_train, y_test = train_test_split(
	train, target, test_size=1 - train_ratio)

# performing test validation split
x_val, x_test, y_val, y_test = train_test_split(
	x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio))

# initializing all the base model objects with default parameters
model_1 = LinearRegression()
model_2 = xgb.XGBRegressor()
model_3 = RandomForestRegressor()

# training all the model on the train dataset

# training first model
model_1.fit(x_train, y_train)
val_pred_1 = model_1.predict(x_val)
test_pred_1 = model_1.predict(x_test)

# converting to dataframe
val_pred_1 = pd.DataFrame(val_pred_1)
test_pred_1 = pd.DataFrame(test_pred_1)

# training second model
model_2.fit(x_train, y_train)
val_pred_2 = model_2.predict(x_val)
test_pred_2 = model_2.predict(x_test)

# converting to dataframe
val_pred_2 = pd.DataFrame(val_pred_2)
test_pred_2 = pd.DataFrame(test_pred_2)

# training third model
model_3.fit(x_train, y_train)
val_pred_3 = model_1.predict(x_val)
test_pred_3 = model_1.predict(x_test)

# converting to dataframe
val_pred_3 = pd.DataFrame(val_pred_3)
test_pred_3 = pd.DataFrame(test_pred_3)

# concatenating validation dataset along with all the predicted validation data (meta features)
df_val = pd.concat([x_val, val_pred_1, val_pred_2, val_pred_3], axis=1)
df_test = pd.concat([x_test, test_pred_1, test_pred_2, test_pred_3], axis=1)

# making the final model using the meta features
final_model = LinearRegression()
final_model.fit(df_val, y_val)

# getting the final output
final_pred = final_model.predict(df_test)

#printing the mean squared error between real value and predicted value
print(mean_squared_error(y_test, pred_final))